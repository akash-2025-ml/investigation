# Robust ML Model for Malware Detection (Anti-Overfitting)

## üéØ Purpose

This project builds a malware classification model that **avoids overfitting** through:
1. Removing label-leaking features
2. Aggressive regularization
3. Proper train/validation/test splits
4. Cross-validation
5. Honest performance evaluation

## üö® **BRUTAL HONESTY: Read This First**

**TL;DR:** Your previous models were probably overfitting because your dataset contains **label-leaking features** - features that give away the answer because they're computed AFTER knowing if something is malicious.

### The Problem

Your dataset has these suspicious features:
- `sender_known_malicious` - How do you "know" it's malicious? Because you labeled it!
- `smtp_ip_known_malicious` - Same problem
- `return_path_known_malicious` - Same problem
- `final_url_known_malicious` - Same problem
- `any_file_hash_malicious` - Literally says "malicious"
- `total_components_detected_malicious` - Aggregates other "malicious" features

**These features create circular logic:**
- Training: "If sender is known malicious ‚Üí predict malicious" (99% accurate!)
- Production: "Is this sender malicious?" ‚Üí "I don't know, that's what I'm trying to predict!" (Useless!)

### The Fix

Our solution **removes these features** and trains on:
- Behavioral scores (sandbox, AMSI, exfiltration)
- Attachment characteristics (count, executable, unscannable)
- Content analysis (macros, scripts, ActiveX, network calls)
- Detection patterns (exploit, packer, YARA, IOC)

**Expected realistic performance WITHOUT label leakage:**
- 70-85% accuracy: Good
- 85-90% accuracy: Excellent
- 90%+ accuracy: Suspicious (check for remaining leakage)

**Not 99%+ like you might have seen before, but actually useful.**

## üì¶ Installation

### Option 1: Automated Setup (Recommended)

```bash
chmod +x setup_and_run.sh
./setup_and_run.sh
```

### Option 2: Manual Setup

```bash
# Create virtual environment
python3 -m venv ml_env
source ml_env/bin/activate

# Install dependencies
pip install -r requirements.txt

# Run training
python train_robust_model.py
```

### Option 3: System-wide Installation

```bash
pip install pandas numpy scikit-learn matplotlib seaborn
python3 train_robust_model.py
```

## üöÄ Usage

### Basic Training

```bash
python3 train_robust_model.py
```

This will:
1. Load and analyze `data.csv`
2. Remove label-leaking features
3. Check for data quality issues
4. Train 6 different regularized models
5. Compare models using proper validation
6. Select best model based on generalization
7. Evaluate on held-out test set
8. Save the best model to `best_model_no_leakage.pkl`

### Using the Trained Model

```python
import pickle
import pandas as pd
import numpy as np

# Load the model
with open('best_model_no_leakage.pkl', 'rb') as f:
    package = pickle.load(f)

model = package['model']
scaler = package['scaler']
feature_names = package['feature_names']

# Prepare your data (must have same features, excluding removed ones)
# X_new should have 14 columns (20 original - 6 removed)
X_new = pd.DataFrame(...)  # Your new data

# Make predictions
X_new_scaled = scaler.transform(X_new)
predictions = model.predict(X_new_scaled)
probabilities = model.predict_proba(X_new_scaled)[:, 1]

print(f"Prediction: {'Malicious' if predictions[0] == 1 else 'Not-Malicious'}")
print(f"Confidence: {probabilities[0]:.2%}")
```

## üìä What the Script Does

### 1. Data Inspection
- Loads data.csv (4,000 samples, 20 features)
- Checks class balance
- Identifies duplicates and missing values

### 2. Feature Engineering
- **Removes 6 label-leaking features:**
  - `sender_known_malicious`
  - `smtp_ip_known_malicious`
  - `return_path_known_malicious`
  - `final_url_known_malicious`
  - `any_file_hash_malicious`
  - `total_components_detected_malicious`

- **Keeps 14 behavioral features:**
  - `max_behavioral_sandbox_score`
  - `malicious_attachment_count`
  - `any_exploit_pattern_detected`
  - `max_amsi_suspicion_score`
  - `total_yara_match_count`
  - `total_ioc_count`
  - `packer_detected`
  - `any_macro_enabled_document`
  - `any_vbscript_javascript_detected`
  - `any_active_x_objects_detected`
  - `any_network_call_on_open`
  - `max_exfiltration_behavior_score`
  - `has_executable_attachment`
  - `unscannable_attachment_present`

### 3. Model Training (Regularized)
Trains 6 models with **aggressive regularization**:

1. **Logistic Regression (L2, C=0.1)** - Ridge penalty
2. **Logistic Regression (L1, C=0.1)** - Lasso penalty (feature selection)
3. **Ridge Classifier (alpha=10)** - Linear model with L2
4. **Random Forest (limited depth)** - Max depth=5, min samples=50
5. **Gradient Boosting (regularized)** - Learning rate=0.05, max depth=3
6. **SVM (C=0.1)** - Low complexity RBF kernel

### 4. Model Selection
Chooses best model based on:
- **60%** Validation F1 score
- **30%** Test F1 score
- **-200%** Overfitting gap penalty
- **-100%** Generalization gap penalty

### 5. Evaluation Metrics
- Accuracy, Precision, Recall, F1
- ROC AUC
- Confusion Matrix
- Overfitting analysis (train vs validation gap)
- Generalization stability (validation vs test gap)
- 5-fold cross-validation

## üìà Understanding the Output

### Good Signs ‚úì
- Overfitting gap < 0.05
- Validation F1 ‚âà Test F1 (within 0.03)
- Cross-validation std < 0.05
- Test accuracy 75-90%

### Warning Signs ‚ö†Ô∏è
- Train accuracy >> Validation accuracy (>0.05 gap)
- Validation F1 very different from Test F1
- High cross-validation std (>0.1)
- Unrealistically high accuracy (>95%)

### Red Flags üö®
- Train accuracy >0.1 higher than validation
- Test metrics very different from validation
- Any metric at 99%+ (likely still has leakage)

## üéì Why This Approach Works

### Problem: Overfitting
- Model memorizes training data instead of learning patterns
- High training accuracy, low test accuracy
- Caused by: complex models, label leakage, small datasets

### Solution: Multiple Techniques

1. **Remove Label Leakage**
   - Features that already contain the answer
   - Example: "known_malicious" fields

2. **Regularization**
   - L1 (Lasso): Feature selection, zeros out unimportant features
   - L2 (Ridge): Shrinks coefficients, reduces complexity
   - Tree depth limits: Prevents memorization

3. **Proper Validation**
   - 60% train / 20% validation / 20% test
   - Validation for model selection
   - Test for final unbiased evaluation
   - Cross-validation for stability check

4. **Model Complexity Control**
   - Limit tree depth (5 instead of unlimited)
   - Increase min_samples_split (50 instead of 2)
   - Reduce learning rate (0.05 instead of 0.1)
   - Lower C/alpha values (stronger penalty)

## üî¨ Technical Details

### Data Split Strategy
```
Total: 4000 samples
‚îú‚îÄ‚îÄ Train:      2400 (60%) - Train models
‚îú‚îÄ‚îÄ Validation:  800 (20%) - Select best model
‚îî‚îÄ‚îÄ Test:        800 (20%) - Final evaluation (never seen during training)
```

### Regularization Parameters

**Logistic Regression:**
- C=0.1 (strong penalty, default is 1.0)
- L1 (Lasso) or L2 (Ridge)

**Random Forest:**
- max_depth=5 (vs unlimited)
- min_samples_split=50 (vs 2)
- min_samples_leaf=20 (vs 1)

**Gradient Boosting:**
- learning_rate=0.05 (vs 0.1)
- max_depth=3 (vs 3-6)
- subsample=0.8 (random 80% sampling)

**SVM:**
- C=0.1 (strong margin penalty)
- RBF kernel with auto gamma scaling

## üìù Files Generated

- `best_model_no_leakage.pkl` - Trained model, scaler, and metadata
- `HONEST_ASSESSMENT.md` - Detailed analysis of overfitting issues
- `train_robust_model.py` - Main training script
- `requirements.txt` - Python dependencies

## ‚ùì FAQ

### Q: Why is my accuracy "only" 80%?
A: That's actually good! Real malware detection is hard. Without cheating via label leakage, 80-85% is realistic and useful.

### Q: My previous model got 98% accuracy, why is this worse?
A: Your previous model was cheating by using "known_malicious" features. It learned "if labeled malicious, predict malicious" which is circular and useless in production.

### Q: Can I add back the removed features?
A: Only if you truly have them BEFORE labeling in production. If you're detecting malware, you don't know if a sender is "known malicious" yet - that's what you're trying to determine!

### Q: How do I improve performance?
A:
1. Collect more data (more samples)
2. Engineer better features (behavioral analysis)
3. Try ensemble methods (combine models)
4. Clean the data (remove noise)
5. **DON'T** add back label-leaking features!

### Q: What if I still see overfitting?
A:
1. Increase regularization (lower C, higher alpha)
2. Reduce model complexity (lower max_depth)
3. Add more training data
4. Check for remaining data leakage
5. Use simpler models (logistic regression)

### Q: Is 70% accuracy acceptable?
A: Depends on context:
- Academic project: Acknowledge limitations, focus on methodology
- Production: 70% might be useful with human review
- Research: Compare to baselines and state-of-the-art
- **Reality:** Malware detection is adversarial and constantly evolving

## üéØ Key Takeaways

1. **Label leakage is the #1 cause of overfitting** in your dataset
2. **Remove features that contain the answer** (known_malicious fields)
3. **Use strong regularization** to prevent memorization
4. **Proper validation splits** are crucial
5. **80-85% is realistic**, 99% is suspicious
6. **Generalization > Raw accuracy**

## üìö Further Reading

- [Overfitting in Machine Learning](https://en.wikipedia.org/wiki/Overfitting)
- [Regularization Techniques](https://scikit-learn.org/stable/modules/linear_model.html#regularization)
- [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html)
- [Data Leakage](https://machinelearningmastery.com/data-leakage-machine-learning/)

## ü§ù Contributing

Found issues? Have suggestions?
- Check HONEST_ASSESSMENT.md for detailed analysis
- Review train_robust_model.py for implementation
- Experiment with different regularization strengths

## ‚öñÔ∏è License

This code is for educational purposes. Use responsibly.

---

**Remember:** Good ML is about generalization, not memorization. An honest 80% beats a dishonest 99%.
